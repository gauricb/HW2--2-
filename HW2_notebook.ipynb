{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce6be02",
   "metadata": {},
   "source": [
    "# CMPSCI 389 HW2a\n",
    "##### Assigned: Feb 22 2023; Due: Mar 1 2023 @ 11:59 pm EST\n",
    "\n",
    "#### Abstract\n",
    "It’s time! We’re making a proper Multi-layer neural network – well, you are at least.\n",
    "\n",
    "Instead of just doing matrix multiplication we’re also going to include non-linearities. This can be a bit of a headache (not gonna lie to you), but if you think about the computation graph and have the patience to read my dumb comments you’ll be fine – I promise! And yes you still need to use Latex, I’m not backing down on that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5a35f14",
   "metadata": {},
   "source": [
    "### 1) Primer – what’s different?\n",
    "So, now that we’re in the realm of multi-layer perceptrons (MLP), things are going to get a little more complicated. In particular we are going to have multiple layers (essentially perceptrons with multiple outputs), which will have weight **matrices**, denoted as ($W_{0}$,$W_{1}$,...,$W_{L}$ for $L$ layers) which will have dimensions of (input size, output size), where the input and output size are the corresponding input and output size of that layer – size is the number of neurons in that layer.\n",
    "\n",
    "For example we could have a two layer MLP which would then have a $W_{0}$ and $W_{1}$. The data given may have $M$ input features and $C$ output features. In this case we know the sizes of the weight matrices must have be $W_{0} = (M, A)$ and $W_{1} = (A, C)$ where A is some number that we choose. The important part of this is that the first layer must have first dimension equal to the size of the input and the final layer must have final dimension equal to the output size. Everything else is up to us – as long as the match. Hope that landed, but I bet it will by the end of the HW.\n",
    "\n",
    "![example neural network](L_Layer_Net.png)\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Figure 1: Example of a L layer neural network – **this is not including a nonlinearity**}\n",
    "\\end{align}\n",
    "\n",
    "The second major change is that we will include a nonlinearity operation, *σ*, after every layer of our MLP, **besides the final layer**. This *σ* must be a nonlinear function and will be taken on the entire output of each layer.\n",
    "\n",
    "Because of these two changes we must make use of backpropogation (finding the gradient layer by layer without repeat- ing calculation) and must also remember the output of each layer of the NN (you’ll see why when we do the computation graph – yeah I’m going to make you do one of those)\n",
    "\n",
    "In the coding part we’re going to refer to the output of a certain layer, *layer*, and its corresponding nonlinearity as *$A_{layer+1}$* . This would make the output of the first ($W_{0}$) layer be represented by $A_{1}$. If you actually read all of this use the *$A_{layer+1}$* notation in the computation graph so I know you’re a ~~tryhard~~ good student."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739184b",
   "metadata": {},
   "source": [
    "### 2) Math! and other stuff that will help you (30 points)\n",
    "Do the math:\n",
    "\n",
    "1. For the following nonlinearity, calculate the derivative of it w.r.t (with respect to) its input (this is kind of tricky, but if you get stuck the answers in lecture (you didn’t hear it from me). Give it a try first though, pretty please):\n",
    "\n",
    "\\begin{align}\n",
    "    σ(X) = \\frac{1}{1+e^{X}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{∂σ}{∂X}= σ(X)(1 - σ(X))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6579a8",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db7aa161",
   "metadata": {},
   "source": [
    "2. Imagining that we’re using this nonlinearity after a single layer of neural network – calculate the derivative of the output w.r.t the weights.\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y} = X \\cdot W + b \\text{  and  } σ(\\hat{y}) = \\frac{1}{1+e^{\\hat{y}}}\n",
    "\\end{align}\n",
    "\n",
    "also, recall:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{∂A}{∂C} = \\frac{∂A}{∂B} * \\frac{∂B}{∂C}\n",
    "\\end{align}\n",
    "\n",
    "solve: \n",
    "\\begin{align}\n",
    "    \\frac{∂σ}{∂W}=  X^T\\cdot(1 - σ(X) (σ(X)) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1496ad",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0713355",
   "metadata": {},
   "source": [
    "3. Now something I’m sure you’ll love – draw a computation graph of a 2 layer neural network (2 weight matrices – using dot product as a basic operation) with nonlinearity, σ, between the layers. I.E :\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y} = (σ(X\\cdot W_{0}+b_{0}))\\cdot W_{1} + b_{1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad47c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbc1af2f",
   "metadata": {},
   "source": [
    "4. Now time to use the computation graph! Use it to solve the following:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{∂\\hat{y}}{∂b_{0}} = σ(X\\cdot W_{0}+b_{0})(1-σ(X\\cdot W_{0}+b_{0}))\\cdot W_{1}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{∂\\hat{y}}{∂W_{0}} = X^T\\cdot((1 - σ(X\\cdot W_{0}+b_{0})) (σ(X\\cdot W_{0}+b_{0})) \\cdot W_1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161a19b",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb2ee1f",
   "metadata": {},
   "source": [
    "### 3) CODING! (60 points)\n",
    "So I’ll level with you – this is gonna be a little bit harder than the last one. But using everything you know you’ll be able to do it cause I believe in you! BTW **you need to pip install a package called tqdm** for this hw\n",
    "\n",
    "Primarily, we have a somewhat strange (at first at least) choice to use a dictionary to store everything. This will become tremendously useful though when it comes to doing backpropogation so just trust me I guess ¯\\\\_(ツ)_/¯. You can just run the first block, then beyond that you'll need to fill out each TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da06227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(file_path: str, label: str)->Tuple[np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    This function loads and parses text file separated by a ',' character and\n",
    "    returns a data set as two arrays, an array of features, and an array of labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "                path to the file containing the data set\n",
    "    label : str\n",
    "                A label of whether to grab training or testing data\n",
    "    Returns\n",
    "    -------\n",
    "    features : ndarray\n",
    "                2D array of shape (n,c) containing features for the data set\n",
    "    labels : ndarray\n",
    "                2D array of shape (n,d) containing labels for the data set\n",
    "    '''\n",
    "    D = np.genfromtxt(file_path, delimiter=\",\")\n",
    "    if label == \"train\":\n",
    "        features = D[0:800, :-3]  # all columns but the last three\n",
    "        labels = D[0:800, -3:]  # the last 3 columns \n",
    "    else:\n",
    "        features = D[801:1000, :-3]  # all columns but the last three\n",
    "        labels = D[801:1000, -3:]  # the last 3 columns\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5898c84",
   "metadata": {},
   "source": [
    "1. **Initialize the network:** complete the *initialize_network()* method. \n",
    "\n",
    "    For this you’ll recieve a list of layer sizes, *layer_sizes*, which will denote the layer sizes with the first being the input size and the last being the output size, this will tell us what sized weight matrices we'll need.\n",
    "\n",
    "    **Example:** layer_size = [100,1], this will denote a 1-layer network (like a Perceptron), with inputs with 100 features and 1 feature output. Here we'd want a (100 x 1) weight matrix.\n",
    "\n",
    "    **Example 2:** layer_size = [100, 20, 1], here we'd want a 2-layer network that took the same size input and generated the same size output, but first multiplies to size 20 first. here we would need to weight matrices (one of size: (100,20) and another of size (20,1))  \n",
    "\n",
    "    For the weight matrices and bias terms initialize using *np.random.randn()* to sample random numbers between [-1, 1] from a normal distribution (or bell curve or gaussian, all the same). Then use the scale value to scale all of them. These need to be stored in the given dictionary such that the first weight matrix is stored as ”W0” and first bias is ”b0”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d1a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(layer_sizes : list, scale : float):\n",
    "    \"\"\"\n",
    "    This function will inialize weights for an arbitrary neural network defined by\n",
    "    the number of neurons in each layer, given by 'layer_sizes'\n",
    "    Your weights should be initialized to be numpy arrays composed of random numbers\n",
    "    taken from a gaussian distribution with mean=0 and std=1, then multiplied by scale\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer_sizes : np.ndarray\n",
    "        An array of intergers that denote the number of neurons in each layer\n",
    "        [100, 50, 20] would denote a network with 100 inputs, a hidden layer\n",
    "        with 50 neurons and output of size 20 -- this would mean W_0 would have dimensions (100,50) for example\n",
    "    scale : float\n",
    "        The scale of our initial weights, our weights should mostly be in the range\n",
    "        [-1,1] * scale\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    init_params : dict\n",
    "        A dictionary that maps labels for parameters to an array of those parameters' \n",
    "        initial values\n",
    "        You MUST have 'W0' map to the first weight matrix, 'W1' to the second, etc. \n",
    "        Hint: \"W\" + str(1) is \"W1\"\n",
    "        AND have the first biases similarly be \"b0\", \"b1\", etc\n",
    "    \"\"\"\n",
    "\n",
    "    init_params = {}\n",
    "\n",
    "    # TODO Initialize the parameter dictionary with weight matrices and biases with random values\n",
    "    # You need to use np.random.randn() to do this -- you can look up the API\n",
    "    # This will give a number sampled from a normal distribution (a bell curve) \n",
    "    num_layers = len(layer_sizes)\n",
    "    for l in range(1, num_layers): \n",
    "        init_params['W' + str(l-1)] = np.random.randn(layer_sizes[l-1], layer_sizes[l]) * scale\n",
    "        init_params['b' + str(l-1)] = np.random.randn(1, layer_sizes[l]) * scale\n",
    "\n",
    "    return init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "348ff4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your output type: <class 'dict'>\n",
      "your output length: 6\n",
      "You should have a dictionary with 6 terms\n",
      "[[ 1.3169555   2.98784195  1.26114499 -1.32753306  0.53268528]\n",
      " [-1.13565866 -0.54943014 -1.3028879  -1.04294705 -0.46252938]\n",
      " [ 0.95382179  0.36197628 -0.1905767   0.84343313  1.52789071]\n",
      " [-0.56512643 -1.01416982  0.21792926 -0.56457657  0.51217639]\n",
      " [ 1.87218888  0.7494944  -1.98703988 -0.01553079  0.47159759]\n",
      " [ 1.03796558  0.58488902 -0.13586021 -0.15773853  1.35819503]\n",
      " [-2.08027778  0.09635716 -1.36317595  2.65127052  0.15838091]\n",
      " [-0.88360055 -0.30525614  2.35623818  0.68213733  0.13652485]\n",
      " [ 1.81957591  1.36541756  1.83283333 -0.63531171 -0.7851656 ]\n",
      " [ 0.12364741 -1.40850117 -0.27562356 -1.5978932  -1.29267819]\n",
      " [-1.09507543  0.24099562  0.41295529  0.18372237 -1.66770899]\n",
      " [-0.23326994 -1.86099011 -0.65743502 -0.6228648  -1.06208625]\n",
      " [-1.65224497  1.33444199  0.96023509 -0.17411175  0.04663985]\n",
      " [-1.54878381  0.72530969  0.75832484 -1.10535135  0.11547741]\n",
      " [-1.43942871 -1.47096534 -1.083037   -0.46308319 -0.28147218]\n",
      " [-1.04485864 -0.43905652  0.48907327 -1.07918161 -0.71217149]\n",
      " [ 0.32874266 -0.01358539  0.07380471 -0.01489631  0.97276892]\n",
      " [ 0.2500168   1.48346001 -1.38407846  1.35079479 -0.43853551]\n",
      " [-0.61854011  1.54917196  0.63742605  0.84237937 -0.48839593]\n",
      " [ 2.03459707  0.29361252  0.1277851  -0.18514387 -0.73791807]]\n",
      "(20, 5)\n",
      "(1, 5)\n",
      "This should be a 20 x 5 numpy matrix with very small values\n",
      "(17, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "exmaple_layers = [100, 50, 20, 5]\n",
    "example_scale = 1\n",
    "your_output = initialize_network(exmaple_layers, example_scale)\n",
    "\n",
    "print(\"your output type:\",type(your_output) )\n",
    "print(\"your output length:\", len(your_output))\n",
    "print(\"You should have a dictionary with 6 terms\")\n",
    "\n",
    "print(your_output['W2'])\n",
    "print(your_output['W2'].shape)\n",
    "print(your_output['b2'].shape)\n",
    "print(\"This should be a 20 x 5 numpy matrix with very small values\")\n",
    "\n",
    "exmaple_layers = [17, 3]\n",
    "your_output = initialize_network(exmaple_layers, example_scale)\n",
    "print(your_output['W0'].shape)\n",
    "print(your_output['b0'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf04c5",
   "metadata": {},
   "source": [
    "2. Next you need to complete the forward for the nonlinearity. Complete the *sigma_forward()* method. The forward is just the calculation of the output of the nonlinear function given some input, IN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7355acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(OUT: np.ndarray):\n",
    "    \"\"\"\n",
    "    performs the nonlinear function (sigmoid) on the given input and returns the result\n",
    "    this is 1/(1 + e^OUT)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    OUT: np.ndarray \n",
    "        The given output of a layer's matrix multiplication\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A: np.ndarray \n",
    "        sigma(OUT), this is the output of your sigmoid function given input, OUT,\n",
    "        'A' and 'OUT' are used because these are the labels we will use in the next step,\n",
    "        for now you can ignore the naming.\n",
    "    \"\"\"\n",
    "\n",
    "    ######################################\n",
    "\n",
    "    # TODO Implement the sigmoid function \n",
    "    \n",
    "    ######################################\n",
    "\n",
    "    A = 1 / (1 + np.exp(-OUT))\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe29d85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_inputs = np.arange(-5,5,0.25)\n",
    "your_output = sigmoid_forward(example_inputs)\n",
    "\n",
    "plt.plot(example_inputs, your_output)\n",
    "plt.title(\"Your sigmoid function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "044e084f",
   "metadata": {},
   "source": [
    "3. After this you can put it together with your model to make your full *forward()* method, which will do a forward pass through your entire model (each layer, including the nonlinearities). It’s important to note that the final layer is not followed by a nonlinearity. You also must save every intermittent value in the cache (again for use later).\n",
    "\n",
    "It is important to not only do the forward pass, but also cache the intermittent values so that we (well, *you* ) can do backpropogation next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9db3237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params: dict, X: np.ndarray):\n",
    "    \"\"\"\n",
    "    This function will perform the forward pass of your backprop algorithm\n",
    "    It will calculate your networks prediction using your parameters and\n",
    "    will keep a cache of all intermittent values (which you need for backprop)\n",
    "    ** YOU MUST COMPLETE THE \"sigmoid_forward()\" method for this part **\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        A dictionary that maps the labels: 'W0', 'W1', etc to their respective\n",
    "        weight matrices -- this is the current state of your params\n",
    "    X : np.ndarray\n",
    "        A 2D numpy array representing input where each row represents a feature vector\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    prediction : np.ndarray\n",
    "        A 1D numpy array holding the predictions of your network given input X\n",
    "    cache : dict\n",
    "        A dictionary that holds all of the intermittent values calculated during your\n",
    "        forward pass of your network (the 'OUT' and 'A'  of each layer), you must have the\n",
    "        keys of this dictionary be of the form \"AL\" and \"OUTL\" where \"AL\" representes the input\n",
    "        to the L-th layer of weights and \"OUT(L+1)\" is the output after multiplying by weights in Layer L . \n",
    "        \n",
    "        X = OUT0 = A0\n",
    "        XW0 + b0 = OUT1\n",
    "        sigmoid(XW0 + b0) = A1\n",
    "        sigmoid(XW0 + b0)W1 + b1 = OUT2\n",
    "        ... \n",
    "\n",
    "        i.e \"OUT0\" will be the key for exactly the input X and \"A0\" will be (as a special case) also X  \n",
    "        generally \"AL\" will be sigma(\"OUTL\")\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "\n",
    "    \n",
    "    A = np.copy(X)\n",
    "    cache[\"OUT0\"] = A\n",
    "    OUT = np.copy(X)\n",
    "    # TODO -- implement the forward pass of your network\n",
    "    # Loop through each layer of the network\n",
    "    for l in range(len(params)//2):\n",
    "        # Retrieve the weights and biases for this layer\n",
    "        Wl = params[\"W\" + str(l)]\n",
    "        bl = params[\"b\" + str(l)]\n",
    "        \n",
    "        # Compute the dot product and add the bias\n",
    "        \n",
    "        Z = np.dot(A, Wl) + bl\n",
    "        OUT = Z\n",
    "        # Compute the sigmoid activation function\n",
    "        Al = sigmoid_forward(Z)\n",
    "        \n",
    "        # Save the intermediate values in the cache dictionary\n",
    "        cache[\"A\" + str(l)] = A\n",
    "        cache[\"OUT\" + str(l+1)] = Z\n",
    "        #cache[\"A\" + str(l+1)] = Al\n",
    "        \n",
    "        # Set the output of this layer as the input to the next layer\n",
    "        A = Al\n",
    "\n",
    "    n = len(params)//2 - 2\n",
    "    # The final output is the prediction of the network\n",
    "    prediction = OUT\n",
    "    \n",
    "\n",
    "    # print(\"cache OUT of last layer: \", cache[f\"OUT{len(params) // 2 - 1}\"])\n",
    "\n",
    "    return prediction, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4de1d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single layer Network ----------------------------\n",
      "\n",
      "your prediction: [[1.81846775 1.81846775 1.81846775 1.81846775]]\n",
      "correct prediction: [[1.81846775 1.81846775 1.81846775 1.81846775]]\n",
      "\n",
      "Now larger Network ----------------------------\n",
      "\n",
      "your ouput size with larger net: (3, 2) -- Should be (3,2)\n",
      "your Cache (check this looks right):\n",
      "\n",
      "OUT0 : [[0.97572353 0.37219616 0.24532187 0.87422672 0.64896484]\n",
      " [0.85751317 0.74802149 0.08998764 0.0289001  0.09404535]\n",
      " [0.11407432 0.92606146 0.78260996 0.540031   0.68010387]]\n",
      "\n",
      "A0 : [[0.97572353 0.37219616 0.24532187 0.87422672 0.64896484]\n",
      " [0.85751317 0.74802149 0.08998764 0.0289001  0.09404535]\n",
      " [0.11407432 0.92606146 0.78260996 0.540031   0.68010387]]\n",
      "\n",
      "OUT1 : [[3.11643311 3.11643311 3.11643311 3.11643311]\n",
      " [1.81846775 1.81846775 1.81846775 1.81846775]\n",
      " [3.04288061 3.04288061 3.04288061 3.04288061]]\n",
      "\n",
      "A1 : [[0.95756553 0.95756553 0.95756553 0.95756553]\n",
      " [0.86038217 0.86038217 0.86038217 0.86038217]\n",
      " [0.95447416 0.95447416 0.95447416 0.95447416]]\n",
      "\n",
      "OUT2 : [[3.83026211 3.83026211]\n",
      " [3.44152867 3.44152867]\n",
      " [3.81789666 3.81789666]]\n"
     ]
    }
   ],
   "source": [
    "# test for forward\n",
    "x = np.random.rand(3, 5)\n",
    "\n",
    "print()\n",
    "print(\"Single layer Network ----------------------------\")\n",
    "print()\n",
    "\n",
    "params = {\"W0\": np.ones((5,4)), \"b0\": np.zeros((4))}\n",
    "\n",
    "your_pred, _ = forward(params, x)\n",
    "\n",
    "print(\"your prediction:\", your_pred[1:2])\n",
    "print(\"correct prediction:\", (x.dot(params[\"W0\"]) + params[\"b0\"])[1:2])\n",
    "\n",
    "print()\n",
    "print(\"Now larger Network ----------------------------\")\n",
    "print()\n",
    "\n",
    "params[\"W1\"] = np.ones((4,2))\n",
    "params[\"b1\"] = np.zeros((2))\n",
    "\n",
    "your_pred, your_cache = forward(params, x)\n",
    "\n",
    "print(\"your ouput size with larger net:\", your_pred.shape, \"-- Should be (3,2)\" )\n",
    "print(\"your Cache (check this looks right):\")\n",
    "for key in your_cache:\n",
    "    print()\n",
    "    print(key,\":\", your_cache[key])\n",
    "\n",
    "\n",
    "# Test with different input shape\n",
    "x = np.random.rand(2, 6)\n",
    "\n",
    "params = {\"W0\": np.ones((6,4)), \"b0\": np.zeros((4)),\n",
    "          \"W1\": np.ones((4,2)), \"b1\": np.zeros((2))}\n",
    "\n",
    "your_pred, _ = forward(params, x)\n",
    "\n",
    "assert your_pred.shape == (2,2), \"Output shape is not correct\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f5f17aa",
   "metadata": {},
   "source": [
    "4. Next we are going to start doing the hard part, derivatives! In this step though all you need to do is complete *sigma_backward()* which will calculate the derivative of sigma with respect to its input (whoa! like the math we did!). This method takes its output as an input – why on earth would it do that?? Check out your math! and if it still doesn’t make sense you should check your math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "344ced73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(A: np.ndarray):\n",
    "    \"\"\"\n",
    "    calculates the derivative of your sigma function give the output of it\n",
    "    Parameters\n",
    "    ----------\n",
    "    A: np.ndarray \n",
    "        sigmoid(OUT), this is the A value (output of the sigma) of \n",
    "        some layer. This is all we need to find dsigma / dOUT believe it or not\n",
    "    Returns\n",
    "    ----------\n",
    "    dsigmoid: np.ndarray\n",
    "        the derivative of sigmoid(OUT) dOUT -- this will use the A value\n",
    "        it will also be very simple\n",
    "    \"\"\"\n",
    "\n",
    "    ######################################\n",
    "\n",
    "    # TODO Implement the derivative of sigmoid \n",
    "\n",
    "    ######################################\n",
    "\n",
    "    dsigmoid = [0] * len(A)\n",
    "            \n",
    "    for i in range(len(A)):\n",
    "        dsigmoid[i] = (1 - A[i]) * A[i]\n",
    "\n",
    "\n",
    "    return np.array(dsigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eb6bb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yours: [0.     0.1875 0.25   0.1875 0.    ]\n",
      "correct: [0, 0.1875, 0.25, 0.1875, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_A = [1, 0.75, 0.5, 0.25, 0]\n",
    "your_dsigmoid = sigmoid_backward(example_A)\n",
    "\n",
    "print(\"yours:\",your_dsigmoid)\n",
    "print(\"correct:\", [0, 0.1875, 0.25, 0.1875, 0])\n",
    "\n",
    "example_inputs = np.arange(0,1,0.05)\n",
    "your_output = sigmoid_backward(example_inputs)\n",
    "\n",
    "plt.plot(example_inputs, your_output)\n",
    "plt.title(\"Your sigmoid_backward function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3919a",
   "metadata": {},
   "source": [
    "5. Well it’s later, time to use the cache! This next step is the heart of your entire homework so it may take a little bit. You are going to complete the *backprop_and_loss()* method which will take your network, your cache, and the true output and calculate the loss as well as the gradient of loss with respect to every parameter (i.e. each ”WL” and ”bL”) and store those gradients in a cache using the same naming convention. **Note:** this uses MEAN squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cba4aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_and_loss(params: dict, prediction: np.ndarray, cache: dict, Y : np.ndarray):\n",
    "\n",
    "    loss = np.mean(np.square(prediction-Y))\n",
    "    \n",
    "    gradient = {}\n",
    "    num_layers = len(params) // 2\n",
    "\n",
    "    L = num_layers\n",
    "    dL_y = 2 * (prediction-Y)\n",
    "    # stored dl_out and dl_da\n",
    "    derivs = {}\n",
    "    derivs['OUT' + str(L)] = dL_y\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        dL_w = np.dot(cache['A'+str(L-1)].transpose(), dL_y)\n",
    "        gradient['W0'] = dL_w\n",
    "        \n",
    "        db0 = np.sum(dL_y, axis=0, keepdims=True)\n",
    "        \n",
    "        gradient['b0'] = np.sum(dL_y, axis=0, keepdims=True)\n",
    "    \n",
    "    if num_layers == 2:\n",
    "        gradient['W1'] = np.dot(cache['A'+str(L-1)].transpose(), dL_y)\n",
    "        gradient['b1'] = np.sum(dL_y, axis=0, keepdims=True)\n",
    "        dL_dA1 = np.dot(dL_y, params['W1'].transpose())\n",
    "        dL_dO1 = dL_dA1 * sigmoid_backward(cache['A'+str(1)])\n",
    "        \n",
    "        gradient['W0'] = np.dot(cache['OUT0'].transpose(), dL_dO1)\n",
    "        gradient['b0'] = np.sum(dL_dO1, axis=0, keepdims=True)\n",
    "\n",
    "    if num_layers > 2:\n",
    "        for l in reversed(range(L)):\n",
    "            dL_dw = np.dot(cache['A'+str(l)].transpose(), derivs['OUT'+str(l+1)])\n",
    "            gradient['W'+str(l)] = dL_dw\n",
    "\n",
    "            dL_db = np.sum(derivs['OUT'+str(l+1)], axis=0, keepdims=True)\n",
    "            gradient['b'+str(l)] = dL_db\n",
    "\n",
    "            if l != 0:\n",
    "                dL_dA = np.dot(derivs['OUT'+str(l+1)], params['W'+str(l)].transpose())\n",
    "                derivs['A'+str(l)] = dL_dA\n",
    "                dL_dO = dL_dA * sigmoid_backward(cache['A'+str(l)])\n",
    "                derivs['OUT'+str(l)] = dL_dO\n",
    "    \n",
    "    return gradient, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc30d722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single layer Network ----------------------------\n",
      "\n",
      "your grad for W0: [[1.88401983]\n",
      " [2.2897339 ]\n",
      " [5.60643954]\n",
      " [4.23857658]\n",
      " [4.97267922]]\n",
      "correct grad for W0: [[1.88401983]\n",
      " [2.2897339 ]\n",
      " [5.60643954]\n",
      " [4.23857658]\n",
      " [4.97267922]]\n",
      "your shape for W0: (5, 1)\n",
      "correct shape for W0: (5, 1)\n",
      "your shape for b0: (1, 1)\n",
      "correct shape for b0: (1, 1)\n",
      "your loss: 2.3528064950224934\n",
      "correct loss: 2.3528064950224934\n"
     ]
    }
   ],
   "source": [
    "#tests for this \n",
    "\n",
    "x = np.random.rand(3, 5)\n",
    "\n",
    "print()\n",
    "print(\"Single layer Network ----------------------------\")\n",
    "print()\n",
    "\n",
    "params = {\"W0\": np.random.rand(5,1), \"b0\": np.random.rand(1)}\n",
    "\n",
    "your_pred, your_cache = forward(params, x)\n",
    "y = np.random.rand(3,1) \n",
    "\n",
    "your_grad, your_loss = backprop_and_loss(params, your_pred, your_cache, y)\n",
    "\n",
    "print(\"your grad for W0:\", your_grad[\"W0\"])\n",
    "print(\"correct grad for W0:\", x.T.dot(2*(your_pred-y)))\n",
    "print(\"your shape for W0:\", your_grad[\"W0\"].shape)\n",
    "print(\"correct shape for W0:\", x.T.dot(2*(your_pred-y)).shape)\n",
    "print(\"your shape for b0:\", your_grad[\"b0\"].shape)\n",
    "print(\"correct shape for b0:\", (1,1))\n",
    "\n",
    "\n",
    "print(\"your loss:\", your_loss)\n",
    "print(\"correct loss:\", np.mean((your_pred-y)**2) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102de34",
   "metadata": {},
   "source": [
    "6. Now it’s time to actually do the learning! You must complete the *gradient_descent()* method which will call all of your other methods to find first the prediction, then the loss and gradient. It will then use them to do gradient descent on the entire parameter dictionary (using the corresponding gradients). You’re free to do this however you see fit (as long as it works!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "844e0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X : np.ndarray, Y : np.ndarray, initial_params : dict, lr : float, num_iterations : int)->Tuple[List[float], np.ndarray]:\n",
    "    \"\"\"\n",
    "    This function runs gradient descent for a fixed number of iterations on the\n",
    "    mean squared loss for a linear model parameterized by the weight vector w.\n",
    "    This function returns a list of the losses for each iteration of gradient\n",
    "    descent along with the final weight vector.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        A 2D numpy array representing input where each row represents a feature vector\n",
    "    Y : np.ndarray\n",
    "        A 1D numpy array where each element represents a label for MSE\n",
    "    initial_params : dictionary\n",
    "        A dictionary holding the initialization of all parameters of the model as np.ndarrays\n",
    "        (e.g. key 'W0' maps to the first weight array of the neural net) \n",
    "    lr : float\n",
    "        The step-size parameter to use with gradient descent.\n",
    "    num_iterations : int\n",
    "        The number of iterations of gradient descent to run.\n",
    "    Returns\n",
    "    -------\n",
    "    losses : list\n",
    "        A list of floats representing the loss from each iteration and the\n",
    "        loss of the final weight vector\n",
    "    final_params : dictionary \n",
    "        A dictionary holding all of the parameters after training as np.ndarrays\n",
    "        (this should have the same mapping as initial_params, just with updated arrays) \n",
    "    \"\"\"\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    final_params = initial_params.copy()\n",
    "    \n",
    "    for n in tqdm(range(num_iterations)):  #tqdm will create a loading bar for your loop\n",
    "        \n",
    "        # TODO Complete this function. It's the whole sh-bang (Gradient Descent)\n",
    "\n",
    "        #forward pass\n",
    "        prediction, cache = forward(final_params, X)\n",
    "\n",
    "        gradient, loss = backprop_and_loss(final_params, prediction, cache, Y)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        for key in final_params:\n",
    "            \n",
    "            final_params[key] = final_params[key] - (lr * gradient[key])\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    return losses, final_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfc56d9a",
   "metadata": {},
   "source": [
    "7. Now you can run it! And once you fix the 1000 bugs that you have... you can run it again and hopefully you see something *somewhat* like below – if you do, congrats! You (probably) have it working:\n",
    "\n",
    "<img style=\"display: block; margin: auto;\"\n",
    "src=\"ex_training.png\">\n",
    "<!-- ![alt text](ex_training.png#center) -->\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Figure 2: Example Learning curve plot with default hyperparameters.}\n",
    "\\end{align}\n",
    "\n",
    "Run both of these blocks and see if your graph looks similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc6da60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(losses: list, names : list):\n",
    "    \"\"\"\n",
    "    This function plots the learning curves for all gradient descent procedures in this homework.\n",
    "    The plot is saved in the file learning_curve.png. No TODO here\n",
    "    Parameters\n",
    "    ----------\n",
    "    losses : list\n",
    "        A list of arrays representing the losses for the gradient at each iteration for each run of gradient descent\n",
    "    names : list\n",
    "        A list of strings representing the names for each gradient descent method\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "    \"\"\"\n",
    "    for loss in losses:\n",
    "        plt.plot(loss)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylim(0, 10000)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Squared Loss\")\n",
    "    plt.title(\"Gradient Descent\")\n",
    "    plt.legend(names)\n",
    "    plt.savefig(\"learning_curve.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8495880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3523.14it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 235.07it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 124.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final training loss values for the first three\n",
      "single layer.........   190.9\n",
      "two layer............   220.1\n",
      "many layer...........   221.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss of model 1: 200.61037780651856\n",
      "test loss of model 2: 237.50296725827448\n",
      "test loss of model 3: 238.8211942660077\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gauribarar/Downloads/HW2 (2)/HW2_notebook.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39msingle layer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtwo layer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmany layer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbest model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     learning_curve(all_losses, names)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m main()\n",
      "\u001b[1;32m/Users/gauribarar/Downloads/HW2 (2)/HW2_notebook.ipynb Cell 32\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest loss of model 3:\u001b[39m\u001b[39m\"\u001b[39m, test_loss2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# TODO choose the hyper parameters for your best model (change them in train_best_model() )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# You'll have to uncomment the below lines for once you find your best model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m best_losses, best_params \u001b[39m=\u001b[39m train_best_model(Train_X, Train_Y) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m best_pred, _ \u001b[39m=\u001b[39m forward(best_params, Test_X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauribarar/Downloads/HW2%20%282%29/HW2_notebook.ipynb#X43sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m best_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39msquare(Test_Y \u001b[39m-\u001b[39m best_pred)) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_best_model' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    Train_X, Train_Y = load_data(\"StudentsPerformance.csv\", \"train\")  # load the data set\n",
    "\n",
    "    N = 1000 # N needs to equal 10,000 for your final plot. You can lower it to tune hyperparameters.\n",
    "\n",
    "    init_params0 = initialize_network([17,3], scale=0.1) # initializes a sigle layer network (perceptron)\n",
    "    losses0, final_params0 = gradient_descent(Train_X, Train_Y, init_params0, lr=1e-6, num_iterations=N)  \n",
    "\n",
    "    init_params1 = initialize_network([17, 5, 3], scale=0.1)  # initializes a two layer network\n",
    "    losses1, final_params1 = gradient_descent(Train_X, Train_Y, init_params1, lr=1e-6, num_iterations=N)  \n",
    "   \n",
    "    init_params2 = initialize_network([17, 7, 3, 3], scale=0.1)  # initializes a many layer network\n",
    "    losses2, final_params2 = gradient_descent(Train_X, Train_Y, init_params2, lr=1e-6, num_iterations=N)   \n",
    "\n",
    "    all_losses = [losses0, losses1, losses2]\n",
    "    #all_losses = [losses0, losses1]\n",
    "    names = [\"single layer\", \"two layer\", \"many layer\"]\n",
    "    print(\"final training loss values for the first three\")\n",
    "    for name, losses in zip(names, all_losses):\n",
    "        print(\"{0:.<21}{1:>8.1f}\".format(name, float(losses[-1])))\n",
    "\n",
    "    learning_curve(all_losses, names)\n",
    "\n",
    "    # TESTING \n",
    "\n",
    "    Test_X, Test_Y = load_data(\"StudentsPerformance.csv\", \"test\")\n",
    "\n",
    "    pred0, _ = forward(final_params0, Test_X)\n",
    "    test_loss0 = np.mean(np.square(Test_Y  - pred0)) \n",
    "    print(\"test loss of model 1:\", test_loss0)\n",
    "\n",
    "    pred1, _ = forward(final_params1, Test_X)\n",
    "    test_loss1 = np.mean(np.square(Test_Y  - pred1)) \n",
    "    print(\"test loss of model 2:\", test_loss1)\n",
    "\n",
    "    pred2, _ = forward(final_params2, Test_X)\n",
    "    test_loss2 = np.mean(np.square(Test_Y  - pred2)) \n",
    "    print(\"test loss of model 3:\", test_loss2)\n",
    "\n",
    "    # TODO choose the hyper parameters for your best model (change them in train_best_model() )\n",
    "    # You'll have to uncomment the below lines for once you find your best model\n",
    "\n",
    "    best_losses, best_params = train_best_model(Train_X, Train_Y) \n",
    "    best_pred, _ = forward(best_params, Test_X)\n",
    "    best_loss = np.mean(np.square(Test_Y - best_pred)) \n",
    "    print(\"train loss of your \\\"best\\\" model:\", float(best_losses[-1]))\n",
    "    print(\"test loss of your \\\"best\\\" model:\", best_loss)\n",
    "\n",
    "    # plot it alongside the other three\n",
    "    all_losses = [losses0, losses1, losses2, best_losses]\n",
    "    names = [\"single layer\", \"two layer\", \"many layer\", \"best model\"]\n",
    "    learning_curve(all_losses, names)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0da1fdc",
   "metadata": {},
   "source": [
    "### 4a) That other bit you forgot about until now (10 points)\n",
    "1. Use *train_best_model()* (you’ll need to find some good hyperparameters) and plot its training alongside the other 3 models (you’ll need to change code in main to plot this) and show it here – Describe your best model and list the values of its hyperparameters here. You should be able to do significantly better than the default, you will be checked for the code you used to find a best model and for the quality of your final loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(Train_X, Train_Y):\n",
    "    \"\"\"\n",
    "    This function will train the model with the hyper parameters\n",
    "    and layers that you have found to be best -- this model must get below 3\n",
    "    MSE loss on our test data (which is not the test data you are given)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO CHANGE THESE VALUES -- you must experiment to find good values\n",
    "    # Maybe you can use a loop to try many!\n",
    "\n",
    "    BEST_SCALE = 0.3              # You need\n",
    "    BEST_LAYERS = [17,1]           # to change\n",
    "    BEST_ALPHA = 1e-4             # these\n",
    "    BEST_NUM_ITERATIONS = 10000    # !\n",
    "\n",
    "    best_params = initialize_network(BEST_LAYERS, BEST_SCALE)\n",
    "    best_losses, best_final_params = gradient_descent(Train_X, Train_Y, best_params, lr=BEST_ALPHA, num_iterations=BEST_NUM_ITERATIONS)\n",
    "\n",
    "    return best_losses, best_final_params  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6306a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14038e05",
   "metadata": {},
   "source": [
    "2. Now note the printed test losses (the loss on the test set which isn’t trained on), for the 3 models and your best, show the train loss and test loss. What is the relationship between these, why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ea208",
   "metadata": {},
   "source": [
    "The train loss and the test loss for the models and the best model is shown above. \n",
    "I think learning rate played an important role in decreasing the loss. The learning rate of the best model was higher than the other 3 models. I observed that the models with less layers are performing better. \n",
    "\n",
    "Also, training loss is less than testing loss is less than testing loss for all 4 models. This might mean that the model is slightly overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142db51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n",
      "sudo: a password is required\n",
      "[NbConvertApp] Converting notebook HW2_notebook.ipynb to html\n",
      "[NbConvertApp] Writing 758572 bytes to HW2_notebook.html\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\n",
    "!jupyter nbconvert --to html HW2_notebook.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4883334075f7ab6e59ce7bd4eae76642d48a00cf7d6194d6a17eece37e74060e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
